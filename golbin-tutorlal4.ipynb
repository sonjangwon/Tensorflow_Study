{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.14087\n",
      "20 1.13861\n",
      "30 1.13637\n",
      "40 1.13416\n",
      "50 1.13197\n",
      "60 1.12979\n",
      "70 1.12765\n",
      "80 1.12552\n",
      "90 1.12341\n",
      "100 1.12133\n",
      "예측값: [0 2 0 0 0 1]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 50.00\n"
     ]
    }
   ],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식을 one-hot 형식의 데이터라고 합니다.\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 신경망은 2차원으로 [입력층(특성), 출력층(레이블)] -> [2, 3] 으로 정합니다.\n",
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# 편향은 아웃풋의 갯수, 즉 최종 결과값의 분류 갯수인 3으로 설정합니다.\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망에 가중치 W과 편향 b을 적용합니다\n",
    "L = tf.add(tf.matmul(X, W), b)\n",
    "# 가중치와 편향을 이용해 계산한 결과 값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU 함수를 적용합니다.\n",
    "L = tf.nn.relu(L)\n",
    "\n",
    "# 마지막으로 softmax 함수를 이용하여 출력값을 사용하기 쉽게 만듭니다\n",
    "# softmax 함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수입니다.\n",
    "# 예) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
    "model = tf.nn.softmax(L)\n",
    "\n",
    "# 신경망을 최적화하기 위한 비용 함수를 작성합니다.\n",
    "# 각 개별 결과에 대한 합을 구한 뒤 평균을 내는 방식을 사용합니다.\n",
    "# 전체 합이 아닌, 개별 결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis 옵션을 사용합니다.\n",
    "# axis 옵션이 없으면 -1.09 처럼 총합인 스칼라값으로 출력됩니다.\n",
    "#        Y         model         Y * tf.log(model)   reduce_sum(axis=1)\n",
    "# 예) [[1 0 0]  [[0.1 0.7 0.2]  -> [[-1.0  0    0]  -> [-1.0, -0.09]\n",
    "#     [0 1 0]]  [0.2 0.8 0.0]]     [ 0   -0.09 0]]\n",
    "# 즉, 이것은 예측값과 실제값 사이의 확률 분포의 차이를 비용으로 계산한 것이며,\n",
    "# 이것을 Cross-Entropy 라고 합니다.\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "# tf.argmax: 예측값과 실제값의 행렬에서 tf.argmax 를 이용해 가장 큰 값을 가져옵니다.\n",
    "# 예) [[0 1 0] [1 0 0]] -> [1 0]\n",
    "#    [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-0f058860f98c>:47: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "10 0.70835\n",
      "20 0.561749\n",
      "30 0.451848\n",
      "40 0.3588\n",
      "50 0.282355\n",
      "60 0.214397\n",
      "70 0.155684\n",
      "80 0.109268\n",
      "90 0.0762864\n",
      "100 0.0531639\n",
      "예측값: [0 1 2 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 100.00\n"
     ]
    }
   ],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "# 신경망의 레이어를 여러개로 구성하여 말로만 듣던 딥러닝을 구성해 봅시다!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2, 10] 으로 정합니다.\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런 갯수, 분류 갯수] -> [10, 3] 으로 정합니다.\n",
    "W2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# b1 은 히든 레이어의 뉴런 갯수로, b2 는 최종 결과값 즉, 분류 갯수인 3으로 설정합니다.\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용합니다\n",
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산합니다.\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냅니다.\n",
    "model = tf.add(tf .matmul(L1, W2), b2)\n",
    "\n",
    "# 텐서플로우에서 기본적으로 제공되는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용 함수를 다음처럼 간단하게 적용할 수 있습니다.\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  10 :  5.54343\n",
      "loss at step  20 :  3.45069\n",
      "loss at step  30 :  3.40049\n",
      "loss at step  40 :  3.36012\n",
      "loss at step  50 :  3.28322\n",
      "loss at step  60 :  2.93227\n",
      "loss at step  70 :  3.22204\n",
      "loss at step  80 :  3.04011\n",
      "loss at step  90 :  3.15526\n",
      "loss at step  100 :  3.232\n",
      "loss at step  110 :  3.27939\n",
      "loss at step  120 :  3.3522\n",
      "loss at step  130 :  2.89849\n",
      "loss at step  140 :  3.17087\n",
      "loss at step  150 :  3.2666\n",
      "loss at step  160 :  3.33687\n",
      "loss at step  170 :  3.35079\n",
      "loss at step  180 :  3.09166\n",
      "loss at step  190 :  3.09672\n",
      "loss at step  200 :  3.27183\n",
      "loss at step  210 :  3.39169\n",
      "loss at step  220 :  3.1997\n",
      "loss at step  230 :  3.22551\n",
      "loss at step  240 :  2.95498\n",
      "loss at step  250 :  3.31166\n",
      "loss at step  260 :  2.91395\n",
      "loss at step  270 :  3.17639\n",
      "loss at step  280 :  3.10328\n",
      "loss at step  290 :  3.03638\n",
      "loss at step  300 :  3.15485\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17b78343ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word2Vec 모델을 간단하게 구현해봅니다.\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# matplot 에서 한글을 표시하기 위한 설정\n",
    "font_name = matplotlib.font_manager.FontProperties(\n",
    "                fname=\"/Library/Fonts/NanumGothic.otf\"  # 한글 폰트 위치를 넣어주세요\n",
    "            ).get_name()\n",
    "matplotlib.rc('font', family=font_name)\n",
    "\n",
    "# 단어 벡터를 분석해볼 임의의 문장들\n",
    "sentences = [\"나 고양이 좋다\",\n",
    "             \"나 강아지 좋다\",\n",
    "             \"나 동물 좋다\",\n",
    "             \"강아지 고양이 동물\",\n",
    "             \"여자친구 고양이 강아지 좋다\",\n",
    "             \"고양이 생선 우유 좋다\",\n",
    "             \"강아지 생선 싫다 우유 좋다\",\n",
    "             \"강아지 고양이 눈 좋다\",\n",
    "             \"나 여자친구 좋다\",\n",
    "             \"여자친구 나 싫다\",\n",
    "             \"여자친구 나 영화 책 음악 좋다\",\n",
    "             \"나 게임 만화 애니 좋다\",\n",
    "             \"고양이 강아지 싫다\",\n",
    "             \"강아지 고양이 좋다\"]\n",
    "\n",
    "# 문장을 전부 합친 후 공백으로 단어들을 나누고 고유한 단어들로 리스트를 만듭니다.\n",
    "word_sequence = \" \".join(sentences).split()\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "# 문자열로 분석하는 것 보다, 숫자로 분석하는 것이 훨씬 용이하므로\n",
    "# 리스트에서 문자들의 인덱스를 뽑아서 사용하기 위해,\n",
    "# 이를 표현하기 위한 연관 배열과, 단어 리스트에서 단어를 참조 할 수 있는 인덱스 배열을 만듭합니다.\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "\n",
    "# 윈도우 사이즈를 1 로 하는 skip-gram 모델을 만듭니다.\n",
    "# 예) 나 게임 만화 애니 좋다\n",
    "#   -> ([나, 만화], 게임), ([게임, 애니], 만화), ([만화, 좋다], 애니)\n",
    "#   -> (게임, 나), (게임, 만화), (만화, 게임), (만화, 애니), (애니, 만화), (애니, 좋다)\n",
    "skip_grams = []\n",
    "\n",
    "for i in range(1, len(word_sequence) - 1):\n",
    "    # (context, target) : ([target index - 1, target index + 1], target)\n",
    "    # 스킵그램을 만든 후, 저장은 단어의 고유 번호(index)로 저장합니다\n",
    "    target = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
    "\n",
    "    # (target, context[0]), (target, context[1])..\n",
    "    for w in context:\n",
    "        skip_grams.append([target, w])\n",
    "\n",
    "\n",
    "# skip-gram 데이터에서 무작위로 데이터를 뽑아 입력값과 출력값의 배치 데이터를 생성하는 함수\n",
    "def random_batch(data, size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
    "\n",
    "    for i in random_index:\n",
    "        random_inputs.append(data[i][0])  # target\n",
    "        random_labels.append([data[i][1]])  # context word\n",
    "\n",
    "    return random_inputs, random_labels\n",
    "\n",
    "\n",
    "#########\n",
    "# 옵션 설정\n",
    "######\n",
    "# 학습을 반복할 횟수\n",
    "training_epoch = 300\n",
    "# 학습률\n",
    "learning_rate = 0.1\n",
    "# 한 번에 학습할 데이터의 크기\n",
    "batch_size = 20\n",
    "# 단어 벡터를 구성할 임베딩 차원의 크기\n",
    "# 이 예제에서는 x, y 그래프로 표현하기 쉽게 2 개의 값만 출력하도록 합니다.\n",
    "embedding_size = 2\n",
    "# word2vec 모델을 학습시키기 위한 nce_loss 함수에서 사용하기 위한 샘플링 크기\n",
    "# batch_size 보다 작아야 합니다.\n",
    "num_sampled = 15\n",
    "# 총 단어 갯수\n",
    "voc_size = len(word_list)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# tf.nn.nce_loss 를 사용하려면 출력값을 이렇게 [batch_size, 1] 구성해야합니다.\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# word2vec 모델의 결과 값인 임베딩 벡터를 저장할 변수입니다.\n",
    "# 총 단어 갯수와 임베딩 갯수를 크기로 하는 두 개의 차원을 갖습니다.\n",
    "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "# 임베딩 벡터의 차원에서 학습할 입력값에 대한 행들을 뽑아옵니다.\n",
    "# 예) embeddings     inputs    selected\n",
    "#    [[1, 2, 3]  -> [2, 3] -> [[2, 3, 4]\n",
    "#     [2, 3, 4]                [3, 4, 5]]\n",
    "#     [3, 4, 5]\n",
    "#     [4, 5, 6]]\n",
    "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# nce_loss 함수에서 사용할 변수들을 정의합니다.\n",
    "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "# nce_loss 함수를 직접 구현하려면 매우 복잡하지만,\n",
    "# 함수를 텐서플로우가 제공하므로 그냥 tf.nn.nce_loss 함수를 사용하기만 하면 됩니다.\n",
    "loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, num_sampled, voc_size))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_epoch + 1):\n",
    "        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n",
    "\n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                               feed_dict={inputs: batch_inputs,\n",
    "                                          labels: batch_labels})\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss at step \", step, \": \", loss_val)\n",
    "\n",
    "    # matplot 으로 출력하여 시각적으로 확인해보기 위해\n",
    "    # 임베딩 벡터의 결과 값을 계산하여 저장합니다.\n",
    "    # with 구문 안에서는 sess.run 대신 간단히 eval() 함수를 사용할 수 있습니다.\n",
    "    trained_embeddings = embeddings.eval()\n",
    "\n",
    "\n",
    "#########\n",
    "# 임베딩된 Word2Vec 결과 확인\n",
    "# 결과는 해당 단어들이 얼마나 다른 단어와 인접해 있는지를 보여줍니다.\n",
    "######\n",
    "for i, label in enumerate(word_list):\n",
    "    x, y = trained_embeddings[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
